# -*- coding: utf-8 -*-
"""HousingPrices.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xQ7fTKynlDpfwVWuO6Uq8Tayt7AUzEG3

#Libraries
"""

!pip install pyspark

from mpl_toolkits.mplot3d import Axes3D
from sklearn.preprocessing import StandardScaler
import seaborn as sns

#for plotting data
import matplotlib.pyplot as plt

#for linear algebra
import numpy as np

# accessing directory structure

import os
import pandas as pd


import pickle

from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession, SQLContext
from pyspark.sql.types import *
import pyspark.sql.functions as F
from pyspark.sql.functions import udf, col, isnull, when, count, stddev, mean

from pyspark.ml.regression import LinearRegression
from pyspark.mllib.evaluation import RegressionMetrics

from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.evaluation import RegressionEvaluator

"""# Data Pre-Processing"""

#creating spark session
spark = SparkSession \
    .builder \
    .appName('Housing Data') \
    .getOrCreate()

spark

#reading the csv file
df = (spark.read
      .format("csv")
      .option('header', 'true')
      .load("/content/housingData.csv"))

df.printSchema()
#original data types of columns are string, which are needed to be converted to float

columnList = [x[0] for x in df.dtypes] #creating a list with all column names
# print(columnList)

#changing datatype to float
for var in columnList:
  df = df.withColumn(var, df[var].cast('float'))

#show new datatype of columns
pd.DataFrame(df.dtypes, columns = ['Column Name', 'Data Type'])

df.head(5)

df.count()

#checking null values
df.select([count(when(isnull(c),c)).alias(c) for c in df.columns]).show()

"""There are no missing values in our current dataset"""

numericFeatures = df.toPandas().select_dtypes(include=[np.number])
categoricalFeatures = df.toPandas().select_dtypes(exclude=[np.number])
print("Number of Numerical Features : {}".format(numericFeatures.shape[1]))
print("Number of Categorical Features : {}".format(categoricalFeatures.shape[1]))

"""There are no Categorical Features in our current dataset, only Numeric Features"""

#check number amd percentage of unique values

for column in df.columns:
  print(column, format(df.select(column).distinct().count(), 'd'),
        '%.2f'%(df.select(column).distinct().count()/df.select(column).count()*100))

df_pd = df.toPandas()

for column in df_pd.columns:
    if column == "Median House Value":
        break
    ax = plt.boxplot(data=df_pd, x = column)
    plt.title(column)
    plt.xlabel(column)
    plt.show()
plt.boxplot(data=df_pd, x = "Median House Value")
plt.xlabel("Median House Value")
plt.title("Median House Value")
plt.show()

def calcBounds(df):
  bounds = {
      c: dict(
          zip(["q1","q3"], df.approxQuantile(c, [0.25, 0.75],0))
      )
      for c in df.columns
  }

  for x in bounds:
    iqr = bounds[x]['q3'] - bounds[x]['q1']
    bounds[x]['min'] = bounds[x]['q1'] - (iqr*1.5)
    bounds[x]['max'] = bounds[x]['q3'] + (iqr*1.5)

  return bounds
bounds = calcBounds(df)


df2=df.select("*", *[
    when(
        col(c)>bounds[c]['max'],
        1
    ).otherwise(when(
        col(c)<bounds[c]['min'],
        -1
    ).otherwise(0)
    ).alias(c+"_out")
    for c in df.columns
])

df2.show(10)

for c in df.columns:
  # median_value = df2.approxQuantile(c, [0.5], 0.01)[0]
  # Replace values where "median_value_out" is 1 with the median value
  if(c=="Median House Value"):
    break
  df2 = df2.withColumn( c, when(col(c+"_out") == 1, bounds[c]['max']).otherwise(when(col(c+'_out')==-1, bounds[c]['min']).otherwise(col(c)) ))
  # Show the updated DataFrame
df2.show()

c = "Median House Value"
df2 = df2.withColumn( c, when(col(c+"_out") != 0, np.nan).otherwise((col(c))) )

df2.select([count(when(col(c+"_out")==1,c)).alias(c) for c in df.columns]).show()

df2.select([count(when(isnull(c),c)).alias(c) for c in df.columns]).show()

df2.columns

df_pd = df2.toPandas()

columns = ['Total Rooms','Total Bedrooms','Population','Households','Median Income']

for column in columns:
    ax = plt.boxplot(data=df_pd, x = column)
    plt.title(column)
    plt.xlabel(column)
    plt.show()

#assigns null value to values above upper limit
def removeUpperLimit(df,var, upperLim):
  dfReturn=df.withColumn(var, when(col(var)>=upperLim, np.nan).otherwise(col(var)) )
  return dfReturn

outColumns = ['Total Rooms', 'Median Income']
upperLimitOut = [5000, 7]

for i in range(0,2):
  df3=removeUpperLimit(df2,outColumns[i],upperLimitOut[i])

df3.show(10)

df3 = df3.withColumnRenamed('Median House Value','Price')

#Data imputation
df3 = df3.na.drop(how="any")

df3.select([count(when(isnull(c),c)).alias(c) for c in df3.columns]).show()

df3.count()

"""#Exploratory Data Analysis"""

sns.distplot(df3.select('Price').toPandas(), color="skyblue")
df3.select(F.skewness('Price'), F.kurtosis('Price')).show()

#Distribution of prices
sns.set_style("darkgrid")
sns.histplot(df3.select('Price').toPandas(), bins = 10)

#Average price of house
import matplotlib.pyplot as plt
df_gb = df3.groupby('Total Rooms').avg().sort('Total Rooms').select(['Total Rooms','avg(Price)'])
df_p = df_gb.toPandas()
plt.figure(figsize = (15, 8))
sns.scatterplot(x = df_p['Total Rooms'], y = df_p['avg(Price)'] )

#Adding a column of per-capita income to the dataframe

df3 = df3.withColumn('per_capita_income', df3['Median Income']*10000/df3['Population'])

g = sns.histplot(df3.select('per_capita_income').toPandas())
g.set(xlim = (0, 500))

#Per-capita-income and prices of the home
df_p = df3.toPandas()
sns.scatterplot(x = df_p['per_capita_income'], y = df_p['Price'])

#A lot of data has near $100 per-capita income - data is skewed towards zero.

#Counting per capita that are less than $100
count_blocks = df3.filter('per_capita_income <  100').count()/df3.select('per_capita_income').count()*100
print("Percentage of blocks below $100 per capita: %2f" % count_blocks)

"""#Feature Extraction"""

# df3.columns
df4 = df3
df4.columns

cols_drop = ['Longitude_out',
 'Latitude_out',
 'Housing Median Age_out',
 'Total Rooms_out',
 'Total Bedrooms_out',
 'Population_out',
 'Households_out',
 'Median Income_out',
 'Median House Value_out']

df4 = df4.drop(*cols_drop)

df4.columns

df_pd = df4.toPandas()
plt.figure(figsize=(10,5))
sns.heatmap(df_pd.corr(), annot=True)

"""#Preparing the Data for the Model"""

X = df4.drop("Price")
y = df4.select("Price")

X.columns
# y.columns

from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml import Pipeline

numerical_cols = ['Longitude',
                'Latitude',
                  'Housing Median Age',
                'Total Rooms',
                'Households',
                'Median Income']

label_col = "Price"

stages = []

assembler = VectorAssembler(inputCols=numerical_cols, outputCol = "features")
stages.append(assembler)

pipeline = Pipeline(stages=stages)

inputModel = pipeline.fit(df4)
transformed_df4 = inputModel.transform(df4)

#standard scaler to scale the data

sc1 = StandardScaler(inputCol = "features", outputCol='sc_features',withStd=True, withMean=True)
sc_model = sc1.fit(transformed_df4)
sc_df = sc_model.transform(transformed_df4)

trainData, testData = sc_df.randomSplit([0.8,0.2],seed=1)
# trainData, testData = transformed_df4.randomSplit([0.7,0.3],seed=1)

sc_feat = sc_df.select('sc_features')

sc_feat.show()

"""#Gradient Boosted Tree"""

from pyspark.ml.regression import GBTRegressor

gbt_regressor = GBTRegressor(featuresCol='sc_features', labelCol=label_col)


pipeline_gbtr = Pipeline(stages=[gbt_regressor])

model_gbtr = pipeline_gbtr.fit(trainData)
predictions_gbtr = model_gbtr.transform(testData)

predictions_gbtr.select('features', label_col, 'prediction').show()

#evaluation
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

rmse_evaluator_gbtr = RegressionEvaluator(labelCol=label_col, predictionCol='prediction', metricName='rmse')
rmse = rmse_evaluator_gbtr.evaluate(predictions_gbtr)

mae_evaluator_gbtr = RegressionEvaluator(labelCol=label_col, predictionCol='prediction', metricName='mae')
mae = mae_evaluator_gbtr.evaluate(predictions_gbtr)


print(f"Root Mean Squared Error (RMSE) on test data: {rmse}")
print(f"Mean Squared Error (MSE) on test data: {rmse**2}")
print(f"Mean Absolute Error (MAE) on test data: {mae}")

"""#Random Forest Regressor"""

from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.evaluation import RegressionEvaluator

rfr = RandomForestRegressor(featuresCol='sc_features',
                            labelCol=label_col,
                            seed=42,
                            numTrees=2000)

pipeline_rfr = Pipeline(stages=[rfr])

model_rfr = pipeline_rfr.fit(trainData)
rfr_prediction = model_rfr.transform(testData)

rfr_prediction.select('features', label_col, 'prediction').show()

from pyspark.ml.evaluation import RegressionEvaluator

evaluator_rfr = RegressionEvaluator(labelCol=label_col, predictionCol='prediction', metricName='rmse')
rmse = evaluator_rfr.evaluate(rfr_prediction)
print(f"Root Mean Squared Error (RMSE) on test data: {rmse}")
print(f"Mean Squared Error (MSE) on test data: {rmse**2}")

"""#Linear Regression"""

from pyspark.ml.regression import LinearRegression

linear_reg = LinearRegression(featuresCol='sc_features', labelCol=label_col)

pipeline_lr = Pipeline(stages=[linear_reg])

model_lr = pipeline_lr.fit(trainData)
predictions_lr = model_lr.transform(testData)

predictions_lr.select('features', label_col, 'prediction').show()

#evaluation

evaluator_lr = RegressionEvaluator(labelCol=label_col, predictionCol='prediction', metricName='rmse')
rmse = evaluator_lr.evaluate(predictions_lr)
print(f"Root Mean Squared Error (RMSE) on test data: {rmse}")
print(f"Mean Squared Error (MSE) on test data: {rmse**2}")

"""#Saving the models"""

#GBT
model_gbtr.save('gbtr_model')

#RFR
model_rfr.save('rfr_model')

#LR|
model_lr.save('lr_model')

# !zip -r /content/gbtr_model.zip /content/gbtr_model

"""#Flask host

"""

!pip install pyngrok

import flask
from flask import Flask, render_template, request
from pyspark.ml import PipelineModel
from pyspark.sql import Row
from pyngrok import ngrok

# Running the flask app
app = Flask(__name__)

#load model
loaded_model_gbtr = PipelineModel.load(r'/content/gbtr_model')


@app.route('/', methods=['GET'])
def home():
    return render_template(r'index.html')

@app.route('/', methods=['POST'])
def predict():
    int_feature = [float(x) for x in request.form.values()]
    featureDict = dict(zip(numerical_cols, int_feature))


    row_df = Row(**featureDict)
    single_df = spark.createDataFrame([row_df])

    assembler = VectorAssembler(inputCols=numerical_cols, outputCol = "sc_features")
    pipeline = Pipeline(stages=[assembler])

    inputModel = pipeline.fit(single_df)
    transformed_single_df = inputModel.transform(single_df)



    prediction_single_row = loaded_model_gbtr.transform(transformed_single_df)

    predicted_value = prediction_single_row.select('prediction').collect()[0]['prediction']


    return render_template('index.html', prediction_text='The Housing price is {}'.format(predicted_value))

if __name__ == '__main__':
    app.run()
    # Run ngrok to expose the app
    public_url = ngrok.connect(port=5000)
    print(" * Running on", public_url)